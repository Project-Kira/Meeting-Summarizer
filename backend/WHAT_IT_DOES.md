# ğŸ¯ Meeting Summarizer Backend - What Does It Do?

## Overview

This is a **real-time meeting transcription and summarization system** that processes live meeting audio transcripts and generates intelligent summaries using a local LLM (Large Language Model).

---

## ğŸ¬ What Problem Does It Solve?

**Problem**: During long meetings, participants often miss key points, decisions, and action items. Manual note-taking is tedious and incomplete.

**Solution**: This system automatically:
- Ingests meeting transcripts in real-time (segment by segment)
- Generates intelligent summaries as the meeting progresses
- Extracts decisions, action items, and key topics
- Provides final comprehensive summaries when meetings end

---

## ğŸ—ï¸ Architecture Overview

```
Meeting Audio â†’ Transcription â†’ Backend API â†’ Processing â†’ Summary
                                     â†“
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  FastAPI     â”‚
                              â”‚  REST API    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“                â†“                â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚PostgreSQLâ”‚    â”‚  Worker  â”‚    â”‚   LLM    â”‚
              â”‚ Database â”‚    â”‚  Queue   â”‚    â”‚Inference â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Core Components

### 1. **API Server** (`app/main.py`)
- **FastAPI** REST API for receiving meeting segments
- **WebSocket** support for real-time updates
- **PostgreSQL LISTEN/NOTIFY** for pub/sub messaging
- Endpoints:
  - `POST /meetings` - Create new meeting
  - `POST /ingest/segment` - Add transcript segment
  - `GET /meetings/{id}/summary` - Get current summary
  - `POST /meetings/{id}/finalize` - Finalize meeting
  - `WebSocket /ws/{id}` - Real-time updates

### 2. **Worker System** (`workers/worker.py`)
- **Background job processor** that runs independently
- Picks up summarization jobs from the queue
- Handles batching and retry logic
- Processes segments through:
  - **Chunker** (`workers/chunker.py`) - Splits long text into manageable chunks
  - **LLM Inference** - Generates summaries
  - **Merger** (`workers/merger.py`) - Combines and deduplicates summaries

### 3. **LLM Inference Service** (`inference/serve.py`)
- Runs **llama-cpp-python** for local LLM inference
- Supports GPU acceleration (CUDA/Metal)
- Generates:
  - Meeting summaries
  - Decisions with confidence scores
  - Action items with owners and due dates
  - Topic extraction

### 4. **Database Layer** (`db/`)
- **PostgreSQL** for persistent storage
- Tables:
  - `meetings` - Meeting metadata
  - `segments` - Transcript segments with timestamps
  - `summaries` - Generated summaries (incremental & final)
  - `jobs` - Background job queue
- Migrations in `db/migrations/`

### 5. **Test Server** (`run_test_server.py`)
- Lightweight development server
- In-memory storage (no PostgreSQL needed)
- Mock LLM responses for quick testing
- Perfect for development and CI/CD

---

## ğŸ”„ How It Works - Complete Flow

### Step 1: Meeting Creation
```bash
POST /meetings
{
  "title": "Q4 Planning Meeting",
  "metadata": {"department": "Engineering"}
}
```
â†’ Creates meeting record in database
â†’ Returns meeting ID

### Step 2: Real-Time Transcript Ingestion
```bash
POST /ingest/segment
{
  "meeting_id": "uuid",
  "speaker": "Alice",
  "timestamp_iso": "2025-11-01T14:00:00Z",
  "text_segment": "Let's discuss Q4 goals..."
}
```
â†’ Stores segment in database
â†’ Triggers background summarization job
â†’ Broadcasts update via WebSocket

### Step 3: Background Processing
1. **Worker picks up job** from queue
2. **Chunker splits text** (if needed, max 2000 tokens)
3. **LLM processes each chunk**:
   - Generates summary
   - Extracts decisions
   - Identifies action items
   - Determines topics
4. **Merger combines results** (removes duplicates)
5. **Summary saved** to database

### Step 4: Incremental Summaries
- As segments are added, summaries are updated
- Type: `"incremental"`
- Client can fetch anytime: `GET /meetings/{id}/summary`

### Step 5: Meeting Finalization
```bash
POST /meetings/{id}/finalize
```
â†’ Marks meeting as complete
â†’ Triggers final summary generation
â†’ Type changes to: `"final"`
â†’ Final summary includes complete analysis

---

## ğŸ“Š Data Models

### Meeting
```json
{
  "id": "uuid",
  "title": "Meeting Title",
  "metadata": {},
  "created_at": "2025-11-01T14:00:00Z",
  "finalized": false
}
```

### Segment
```json
{
  "id": "uuid",
  "meeting_id": "uuid",
  "speaker": "Alice",
  "timestamp": "2025-11-01T14:00:00Z",
  "text": "Meeting content...",
  "sequence": 1
}
```

### Summary
```json
{
  "id": "uuid",
  "meeting_id": "uuid",
  "type": "incremental",  // or "final"
  "content": {
    "summary": "Main points discussed...",
    "decisions": [
      {
        "text": "Decision description",
        "confidence": 0.9
      }
    ],
    "action_items": [
      {
        "text": "Task description",
        "owner": "Alice",
        "due_date_iso": "2025-11-07",
        "confidence": 0.85
      }
    ],
    "topics": [
      {
        "name": "Budget Planning",
        "confidence": 0.95
      }
    ]
  },
  "created_at": "2025-11-01T14:05:00Z"
}
```

---

## ğŸš€ Key Features

### âœ… Real-Time Processing
- Segments processed as they arrive
- WebSocket updates for live dashboard
- No need to wait until meeting ends

### âœ… Intelligent Chunking
- Splits long transcripts into optimal sizes
- Uses sliding window (50% overlap) for context
- Ensures LLM gets proper context

### âœ… Summary Deduplication
- Merges similar summaries
- Removes redundant information
- Confidence-based filtering

### âœ… Scalability
- Background job queue handles load
- Retry logic for failed jobs
- Horizontal scaling ready

### âœ… Local LLM
- No external API costs
- Privacy-first (all data stays local)
- GPU acceleration supported

### âœ… Extensible
- Easy to swap LLM models
- Plugin architecture for processors
- API-first design

---

## ğŸ› ï¸ Technical Stack

- **Language**: Python 3.10+
- **Web Framework**: FastAPI
- **Database**: PostgreSQL (production) / In-memory (testing)
- **LLM**: llama-cpp-python (local inference)
- **Queue**: Redis (optional) or PostgreSQL LISTEN/NOTIFY
- **WebSocket**: FastAPI WebSocket support
- **Testing**: pytest, pytest-asyncio
- **Logging**: Structured JSON logs

---

## ğŸ“ File Structure

```
backend/
â”œâ”€â”€ app/                    # API server
â”‚   â”œâ”€â”€ main.py            # FastAPI application
â”‚   â”œâ”€â”€ notifications.py   # WebSocket & pub/sub
â”‚   â””â”€â”€ setup_logging.py   # Logging configuration
â”‚
â”œâ”€â”€ workers/               # Background processing
â”‚   â”œâ”€â”€ worker.py         # Job processor
â”‚   â”œâ”€â”€ chunker.py        # Text chunking
â”‚   â””â”€â”€ merger.py         # Summary merging
â”‚
â”œâ”€â”€ inference/            # LLM service
â”‚   â””â”€â”€ serve.py         # llama-cpp inference
â”‚
â”œâ”€â”€ db/                   # Database layer
â”‚   â”œâ”€â”€ connection.py    # DB connection pool
â”‚   â”œâ”€â”€ repositories.py  # CRUD operations
â”‚   â”œâ”€â”€ migrate.py       # Migration runner
â”‚   â””â”€â”€ migrations/      # SQL migrations
â”‚
â”œâ”€â”€ models/              # Data models
â”‚   â””â”€â”€ schemas.py       # Pydantic models
â”‚
â”œâ”€â”€ config/              # Configuration
â”‚   â””â”€â”€ settings.py      # Environment settings
â”‚
â”œâ”€â”€ tests/               # Test suite
â”‚   â”œâ”€â”€ unit/           # Unit tests
â”‚   â””â”€â”€ integration/    # Integration tests
â”‚
â”œâ”€â”€ frontend/           # Testing UI
â”‚   â””â”€â”€ index.html     # Web interface
â”‚
â”œâ”€â”€ examples/           # Usage examples
â”‚   â”œâ”€â”€ curl_examples.sh
â”‚   â””â”€â”€ websocket_client.py
â”‚
â”œâ”€â”€ run_test_server.py  # Development server
â”œâ”€â”€ test_cli.sh         # CLI testing script
â”œâ”€â”€ restart_servers.sh  # Server management
â”œâ”€â”€ setup.sh           # Initial setup
â”œâ”€â”€ verify.sh          # Health checks
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md          # Main documentation
```

---

## ğŸ¯ Use Cases

### 1. **Corporate Meetings**
- Board meetings
- Team standups
- Planning sessions
- All-hands meetings

### 2. **Education**
- Lecture transcription
- Student discussions
- Faculty meetings

### 3. **Healthcare**
- Doctor-patient consultations
- Medical rounds
- Team briefings

### 4. **Legal**
- Depositions
- Client meetings
- Case discussions

### 5. **Remote Work**
- Zoom/Teams meeting notes
- Async meeting summaries
- Multi-timezone collaboration

---

## ğŸ” Privacy & Security

- **Local Processing**: All data processed on-premises
- **No External APIs**: LLM runs locally
- **Data Control**: Full control over data storage
- **Audit Trail**: Complete logging of all operations
- **Access Control**: API authentication (extensible)

---

## ğŸ“ˆ Performance

- **Latency**: ~2-5 seconds per segment (depends on LLM)
- **Throughput**: 100+ segments/minute (with worker scaling)
- **Scalability**: Horizontal scaling via worker pool
- **GPU Acceleration**: 5-10x faster with CUDA/Metal

---

## ğŸ§ª Testing

### Development Testing
```bash
./test_cli.sh           # Automated API tests
./run_test_server.py    # Start test server
```

### Frontend Testing
```bash
firefox http://localhost:8080/  # Web UI testing
```

### Production Verification
```bash
./verify.sh  # Check all components
```

---

## ğŸš¦ Getting Started

### 1. Setup
```bash
bash setup.sh
```

### 2. Configure
```bash
cp .env.example .env
# Edit .env with your settings
```

### 3. Run (Development)
```bash
python run_test_server.py
```

### 4. Run (Production)
```bash
# Start all services
docker-compose up -d  # (if using Docker)
# Or manually:
python inference/serve.py &
python workers/worker.py &
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### 5. Test
```bash
./test_cli.sh
```

---

## ğŸ“š Documentation

- **README.md** - Quick start guide
- **DEPLOYMENT.md** - Production deployment
- **PROJECT_SUMMARY.md** - Architecture details
- **TROUBLESHOOTING.md** - Common issues

---

## ğŸ“ Summary

**In Simple Terms**: This backend is a smart meeting assistant that:
1. Listens to meeting transcripts (text input)
2. Uses AI to understand what's being said
3. Creates summaries highlighting:
   - What was discussed
   - What was decided
   - What needs to be done
   - Who's responsible
4. Updates summaries in real-time
5. Provides final comprehensive summary when meeting ends

**Key Innovation**: Unlike services that require external APIs (OpenAI, etc.), this runs entirely on your own hardware with local LLM models, giving you complete privacy and control.

**Perfect For**: Organizations that need automatic meeting documentation without sending data to third-party services.
